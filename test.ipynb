{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:/s\\\\abd\\\\abc\\\\d'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('E:/s', 'abd', 'abc', 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ses01F_impro01_F000.wav',\n",
       " 'Ses01F_impro01_F001.wav',\n",
       " 'Ses01F_impro01_F002.wav',\n",
       " 'Ses01F_impro01_F003.wav',\n",
       " 'Ses01F_impro01_F004.wav',\n",
       " 'Ses01F_impro01_F005.wav',\n",
       " 'Ses01F_impro01_F006.wav',\n",
       " 'Ses01F_impro01_F007.wav',\n",
       " 'Ses01F_impro01_F008.wav',\n",
       " 'Ses01F_impro01_F009.wav',\n",
       " 'Ses01F_impro01_F010.wav',\n",
       " 'Ses01F_impro01_F011.wav',\n",
       " 'Ses01F_impro01_F012.wav',\n",
       " 'Ses01F_impro01_F013.wav',\n",
       " 'Ses01F_impro01_F014.wav',\n",
       " 'Ses01F_impro01_F015.wav',\n",
       " 'Ses01F_impro01_M000.wav',\n",
       " 'Ses01F_impro01_M001.wav',\n",
       " 'Ses01F_impro01_M002.wav',\n",
       " 'Ses01F_impro01_M003.wav',\n",
       " 'Ses01F_impro01_M004.wav',\n",
       " 'Ses01F_impro01_M005.wav',\n",
       " 'Ses01F_impro01_M006.wav',\n",
       " 'Ses01F_impro01_M007.wav',\n",
       " 'Ses01F_impro01_M008.wav',\n",
       " 'Ses01F_impro01_M009.wav',\n",
       " 'Ses01F_impro01_M010.wav',\n",
       " 'Ses01F_impro01_M011.wav',\n",
       " 'Ses01F_impro01_M012.wav',\n",
       " 'Ses01F_impro01_M013.wav']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"E:\\\\Download\\\\IEMOCAP_full_release_withoutVideos\\\\IEMOCAP_full_release\\\\Session1\\\\sentences\\\\wav\\\\Ses01F_impro01\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asbdiasf'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"asbdiasf.wav\"[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "b = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.mfcc_extraction import IEMOCAP_Dataset\n",
    "\n",
    "ROOT = \"E:\\\\Download\\\\IEMOCAP_full_release_withoutVideos\\\\IEMOCAP_full_release\"\n",
    "iemocap = IEMOCAP_Dataset(root=ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ang', 'dis', 'exc', 'fea', 'fru', 'hap', 'neu', 'oth', 'sad',\n",
       "       'sur', 'xxx'], dtype='<U3')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemocap.LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Download\\\\IEMOCAP_full_release_withoutVideos\\\\IEMOCAP_full_release\\\\Session1\\\\sentences\\\\wav\\\\Ses01F_impro01\\\\Ses01F_impro01_F000.wav'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iemocap.utterance_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iemocap.utterance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iemocap.label_map.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31 -31 -23 ... -15 -11 -42]\n",
      "[0.0000000e+00 6.2500000e-05 1.2500000e-04 ... 1.3822500e+00 1.3823125e+00\n",
      " 1.3823750e+00]\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "import python_speech_features as psf\n",
    "\n",
    "PATH = iemocap.utterance_paths[1]\n",
    "with wave.open(PATH, 'r') as f:\n",
    "    nchannels, sampwidth, framerate, wav_length = f.getparams()[:4]\n",
    "    data = np.frombuffer(f.readframes(wav_length), dtype=np.short)\n",
    "    print(data)\n",
    "    time = np.arange(0,wav_length) * (1.0/framerate)\n",
    "    print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(137, 40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(137, 40)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mel_spec = psf.logfbank(data, framerate, nfilt=40)\n",
    "delta1 = psf.delta(mel_spec, 2)\n",
    "delta2 = psf.delta(delta1, 2)\n",
    "display(mel_spec.shape)\n",
    "display(delta1.shape)\n",
    "display(delta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 3, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack((mel_spec, delta1, delta2), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ses01'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Ses01F_impro01_F000'[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, array([[[2, 2, 4],\n",
       "                  [0, 2, 1],\n",
       "                  [0, 3, 0]],\n",
       "\n",
       "                 [[1, 2, 0],\n",
       "                  [2, 0, 3],\n",
       "                  [0, 4, 3]],\n",
       "\n",
       "                 [[0, 1, 3],\n",
       "                  [0, 2, 3],\n",
       "                  [3, 1, 0]]])], dtype=object)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.randint(5, size=(3,3,3))\n",
    "b = 1\n",
    "\n",
    "np.array((b, a), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.load(\"E:\\\\Download\\\\IEMOCAP_full_release_withoutVideos\\\\data\\\\train\\\\Ses01F_impro01_F000.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194, 3, 40)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3, 40)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pad(sample[1], ((0, 106), (0, 0), (0, 0))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([194, 3, 40])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sample_data = torch.tensor(sample[1])\n",
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300, 3, 40])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.pad(sample_data, (0, 0, 0, 0, 0, 300 - 194)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/IEMOCAP.pkl\", \"rb\") as f:\n",
    "\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 300, 40, 3)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 300, 40, 3)\n",
      "(1200, 1)\n",
      "(420, 300, 40, 3)\n",
      "(259, 1)\n",
      "(436, 300, 40, 3)\n",
      "(298, 1)\n",
      "(436, 1)\n",
      "(420, 1)\n",
      "(259,)\n",
      "(298,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    \n",
    "    print(data[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 300, 40, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class acrnn(nn.Module):\n",
    "    def __init__(self, num_classes=4, is_training=True,\n",
    "                 L1=128, L2=256, cell_units=128, num_linear=768,\n",
    "                 p=10, time_step=150, F1=64, dropout_keep_prob=1):\n",
    "        super(acrnn, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.is_training = is_training\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "        self.cell_units = cell_units\n",
    "        self.num_linear = num_linear\n",
    "        self.p = p\n",
    "        self.time_step = time_step\n",
    "        self.F1 = F1\n",
    "        self.dropout_prob = 1 - dropout_keep_prob\n",
    "\n",
    "        # tf filter : [filter_height, filter_width, in_channels, out_channels]\n",
    "        self.conv1 = nn.Conv2d(3, self.L1, (5, 3), padding=(2, 1))       # [5, 3,   3, 128]  \n",
    "        self.conv2 = nn.Conv2d(self.L1, self.L2, (5, 3), padding=(2, 1)) # [5, 3, 128, 256]\n",
    "        self.conv3 = nn.Conv2d(self.L2, self.L2, (5, 3), padding=(2, 1)) # [5, 3, 256, 256]\n",
    "        self.conv4 = nn.Conv2d(self.L2, self.L2, (5, 3), padding=(2, 1)) # [5, 3, 256, 256]\n",
    "        self.conv5 = nn.Conv2d(self.L2, self.L2, (5, 3), padding=(2, 1)) # [5, 3, 128, 256]\n",
    "        self.conv6 = nn.Conv2d(self.L2, self.L2, (5, 3), padding=(2, 1)) # [5, 3, 128, 256]\n",
    "\n",
    "        self.linear1 = nn.Linear(self.p*self.L2, self.num_linear) # [10*256, 768]\n",
    "        self.bn = nn.BatchNorm1d(self.num_linear)\n",
    "\n",
    "        self.relu = nn.LeakyReLU(0.01)\n",
    "        self.dropout = nn.Dropout2d(p=self.dropout_prob)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=self.num_linear, hidden_size=self.cell_units, \n",
    "                            batch_first=True, num_layers=1, bidirectional=True) \n",
    "\n",
    "        # for attention\n",
    "        self.a_fc1 = nn.Linear(2*self.cell_units, 1)  \n",
    "        self.a_fc2 = nn.Linear(1, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(2*self.cell_units, self.F1) # [2*128, 64]\n",
    "        self.fc2 = nn.Linear(self.F1, self.num_classes) # [num_classes]\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        layer1 = self.relu(self.conv1(x))\n",
    "        layer1 = F.max_pool2d(layer1, kernel_size=(2, 4), stride=(2, 4))   # [1,2,4,1], padding = 'valid'\n",
    "        layer1 = self.dropout(layer1)\n",
    "\n",
    "        layer2 = self.relu(self.conv2(layer1))\n",
    "        layer2 = self.dropout(layer2)\n",
    "        \n",
    "        layer3 = self.relu(self.conv3(layer2))\n",
    "        layer3 = self.dropout(layer3)\n",
    "\n",
    "        layer4 = self.relu(self.conv4(layer3))\n",
    "        layer4 = self.dropout(layer4)\n",
    "\n",
    "        layer5 = self.relu(self.conv5(layer4))\n",
    "        layer5 = self.dropout(layer5)\n",
    "\n",
    "        layer6 = self.relu(self.conv6(layer5))\n",
    "        layer6 = self.dropout(layer6)\n",
    "        \n",
    "        # lstm\n",
    "        layer6 = layer6.permute(0, 2, 3, 1)\n",
    "        layer6 = layer6.reshape(-1, self.time_step, self.L2*self.p)        # (-1, 150, 256*10)\n",
    "        layer6 = layer6.reshape(-1, self.L2*self.p)                        # (1500, 2560)\n",
    "\n",
    "        linear1 = self.relu(self.bn(self.linear1(layer6)))                 # [1500, 768]\n",
    "        linear1 = linear1.reshape(-1, self.time_step, self.num_linear)     # [10, 150, 768]\n",
    "        \n",
    "        outputs1, output_states1 = self.rnn(linear1)                       # outputs1 : [10, 150, 128] (B,T,D)\n",
    "\n",
    "        # # attention\n",
    "        v = self.sigmoid(self.a_fc1(outputs1))                  # (10, 150, 1)\n",
    "        alphas = self.softmax(self.a_fc2(v).squeeze())          # (B,T) shape, alphas are attention weights\n",
    "        gru = (alphas.unsqueeze(2) * outputs1).sum(axis=1)      # (B,D)\n",
    "        \n",
    "        # # fc\n",
    "        fully1 = self.relu(self.fc1(gru))\n",
    "        fully1 = self.dropout(fully1)\n",
    "        Ylogits = self.fc2(fully1)\n",
    "        Ylogits = self.softmax(Ylogits)\n",
    "\n",
    "        return Ylogits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 300, 40])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# data_194 = torch.randint(0, 5, size=(10, 194, 40, 3))\n",
    "# data_194.transpose((0, 3, 1, 2))\n",
    "data_194 = torch.tensor(np.random.randint(3, size=(10, 300, 40, 3)).transpose((0, 3, 1, 2))).float()\n",
    "data_194.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp/counterfactual_fairness_emotional_recognition'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.datasets import IEMOCAPSamples\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "PATH = \"data/data\"\n",
    "iemocap_samples = IEMOCAPSamples(PATH)\n",
    "iemocap_loader = DataLoader(iemocap_samples, batch_size=64, collate_fn=iemocap_samples.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1154"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iemocap_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_lengths, y = next(iter(iemocap_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 64, 3, 40])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.float()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:20<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "min_padded_length = 1000\n",
    "n = 0\n",
    "\n",
    "loader_iterator = iter(iemocap_loader)\n",
    "sample = None\n",
    "\n",
    "for i in tqdm(range(len(iemocap_loader)), total=len(iemocap_loader)):\n",
    "\n",
    "    X, _, _ = next(loader_iterator)\n",
    "    if X.shape[0] < min_padded_length:\n",
    "        min_padded_length = X.shape[0]\n",
    "\n",
    "    if X.shape[0] < 300:\n",
    "        sample = X\n",
    "\n",
    "print(min_padded_length, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 64, 3, 40])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "class ACRNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ACRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Sequential(*[\n",
    "            nn.Conv2d(in_channels=3,   out_channels=128, kernel_size=(5, 3), padding=(2, 1), stride=(2, 2)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(5, 3), padding=(2, 1), stride=(2, 2)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU()\n",
    "        ])\n",
    "\n",
    "        self.linear = nn.Sequential(*[\n",
    "            nn.Linear(2560, 768)\n",
    "        ])\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=768, hidden_size=256, num_layers=4, dropout=0.5, bidirectional=True)\n",
    "\n",
    "        # self.classifier = nn.Sequential(*[\n",
    "        #     nn.Linear(512, 1024),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(1024, 11),\n",
    "        #     nn.Softmax(dim=2)\n",
    "        # ])\n",
    "\n",
    "        self.a_fc1 = nn.Linear(512, 1)\n",
    "        self.a_fc2 = nn.Linear(1, 1)\n",
    "        self.a_sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.a_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.classifier = nn.Sequential(*[\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0),\n",
    "            nn.Linear(512, 11),\n",
    "            nn.Softmax(dim=1)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, X, X_lengths):\n",
    "\n",
    "        X = X.permute((1, 2, 0, 3))\n",
    "        out = self.embedding(X)\n",
    "        out = out.permute((2, 0, 1, 3))\n",
    "\n",
    "        out_shape = out.shape\n",
    "        out = out.reshape((out_shape[0], out_shape[1], -1))\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # packed_out = pack_padded_sequence(out, X_lengths, enforce_sorted=False)\n",
    "        out, (_, _) = self.rnn(out)\n",
    "        # out, lengths = pad_packed_sequence(out)\n",
    "        # out = out.permute(1, 0, 2).reshape((out.shape[0], -1))\n",
    "        # out = self.classifier(out)\n",
    "\n",
    "        out = out.permute(1, 0, 2)\n",
    "        v = self.a_sigmoid(self.a_fc1(out))\n",
    "        alphas = self.a_softmax(self.a_fc2(v).squeeze())\n",
    "        gru = (alphas.unsqueeze(2) * out).sum(axis=1)\n",
    "\n",
    "        out = self.classifier(gru)\n",
    "\n",
    "        # out = torch.stack([out[X_lengths[i] - 1, i, :] for i in range(out.shape[1])])\n",
    "\n",
    "        return out\n",
    "\n",
    "model = ACRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(X, X_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 11])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_reshape = out.reshape((723, 5, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_reshape[0, 1, 79] == out[0, 1, 1, 39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.datasets import IEMOCAPSamples, IEMOCAPSmall\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "PATH = \"data/data\"\n",
    "iemocap_samples = IEMOCAPSmall()\n",
    "iemocap_loader = DataLoader(iemocap_samples, batch_size=64)\n",
    "\n",
    "X, y = next(iter(iemocap_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 300, 40, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/autodl-tmp/counterfactual_fairness_emotional_recognition'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"data/IEMOCAP_val.pkl\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(856, 300, 40, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = data[2][:428]\n",
    "valid_data.shape\n",
    "\n",
    "np.save('data/IEMOCAP_valid_data.npy', valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_label = data[1][:428]\n",
    "valid_label.shape\n",
    "\n",
    "np.save('data/IEMOCAP_valid_label.npy', valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_label = data[1][:428]\n",
    "valid_label.shape\n",
    "\n",
    "np.save('data/IEMOCAP_Valid_label.npy', valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pernums_valid = np.ones(428, dtype=int)\n",
    "pernums_valid.shape\n",
    "np.save('data/IEMOCAP_pernums_valid.npy', pernums_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.empty((0, 50))\n",
    "b = np.ones((1, 50))\n",
    "np.vstack((a, b)) == b"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ab0d968002a578cf1e1aa041721a175249ba338f6da29efa5ef1a380c630376"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
